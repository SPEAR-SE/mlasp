{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The purpose of this notebook is to find out what's the smallest network \n",
    "# that can be used to get max prediction score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"standardScalerSetup.ipynb\"\n",
    "#loads all the preprocessing libraries and prepares the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Nets imports\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten \n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, Adadelta, Adagrad, Adamax, SGD\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verboseLevel=0\n",
    "validationSplit=0.2\n",
    "batchSize=30\n",
    "epochs=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback preparation\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.5,\n",
    "                              patience=2,\n",
    "                              verbose=verboseLevel,\n",
    "                              mode='min',\n",
    "                              min_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputSize = 9\n",
    "colList = ['HiddenLayers', 'R2Score', 'MAE', 'MSE', 'H5FileName', 'TrainHistory', 'TrainPredictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Large Dataset first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(dataFrame, layerSize, loops, y_train, X_train, y_test, X_test, targetScaler, labelSet):\n",
    "    print(f'Creating model using layer size = {layerSize} on set = {labelSet}.\\n')\n",
    "    for i in range(loops):\n",
    "        print(f'Training on {i} hidden layers\\n')\n",
    "        model = Sequential()\n",
    "        model.add(Dense(layerSize, kernel_initializer='normal',\n",
    "                        #kernel_regularizer=l1(0.01), bias_regularizer=l1(0.01),\n",
    "                        kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01),\n",
    "                        #kernel_regularizer=l1_l2(0.01), bias_regularizer=l1_l2(0.01),\n",
    "                        input_dim=inputSize, activation=custom_activation))\n",
    "        for j in range(i):\n",
    "            model.add(Dense(layerSize, \n",
    "                            #kernel_regularizer=l1(0.01), bias_regularizer=l1(0.01),\n",
    "                            kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01),\n",
    "                            #kernel_regularizer=l1_l2(0.01), bias_regularizer=l1_l2(0.01),\n",
    "                            kernel_initializer='normal', activation=custom_activation))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(1, kernel_initializer='normal', \n",
    "                        #kernel_regularizer=l1(0.01), bias_regularizer=l1(0.01),\n",
    "                        kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01),\n",
    "                        #kernel_regularizer=l1_l2(0.01), bias_regularizer=l1_l2(0.01),\n",
    "                        activation='linear'))\n",
    "\n",
    "        # only set this if GPU capabilities available\n",
    "        # model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "        optmzr=Adam(lr=0.001)    \n",
    "        model.compile(optimizer=optmzr, loss='mae', metrics=['mae'])\n",
    "\n",
    "        model_h5_name = 'mlp_' + str(layerSize)+ '_' + str(i) + '_model_std_' + labelSet + '_L2.h5'\n",
    "        checkpoint_nn_std = ModelCheckpoint(model_h5_name,\n",
    "                                 monitor='val_loss',\n",
    "                                 verbose=verboseLevel,\n",
    "                                 save_best_only=True,\n",
    "                                 mode='min')\n",
    "        callbacks_list_nn_std = [checkpoint_nn_std, reduce_lr]\n",
    "\n",
    "        history_MLP_std = model.fit(X_train.to_numpy(), y_train,\n",
    "                                    batch_size=batchSize, \n",
    "                                    validation_split=validationSplit, \n",
    "                                    epochs=epochs, verbose=verboseLevel,\n",
    "                                    callbacks=callbacks_list_nn_std)\n",
    "\n",
    "        #reload the best model!\n",
    "        model_new = load_model(model_h5_name)\n",
    "        #Predict\n",
    "        y_pred_scaled = model_new.predict(X_test.to_numpy())\n",
    "        #Evaluate metrics\n",
    "        y_pred = targetScaler.inverse_transform(y_pred_scaled)\n",
    "        r2_score = metrics.r2_score(y_test, y_pred)\n",
    "        mae = metrics.mean_absolute_error(y_test, y_pred)\n",
    "        mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "        #store values\n",
    "        row = [i, r2_score, mae, mse, model_h5_name, history_MLP_std, y_pred]\n",
    "        df = pd.DataFrame(np.array(row).reshape(1, len(colList)), columns=colList)\n",
    "        dataFrame = dataFrame.append(df, ignore_index=True)\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        del(model)\n",
    "        del(model_new)\n",
    "        \n",
    "    return dataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataFrame = pd.DataFrame(columns=colList)\n",
    "layerSize = 64\n",
    "loops = 15\n",
    "dataFrame = createModel(dataFrame, layerSize, loops, \n",
    "                        y_train_scaled_std_all, X_train_all_std,\n",
    "                        y_test_all_std, X_test_all_std, \n",
    "                        targetStdScalerAll, 'all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot train vs validation\n",
    "plt.figure(figsize=(20,10))\n",
    "#plt.plot(dataFrame['R2Score'])\n",
    "plt.plot(dataFrame['MAE'])\n",
    "#plt.plot(dataFrame['MSE'])\n",
    "plt.title('Training Scores MLP')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Iteration')\n",
    "plt.legend(['MAE'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the IDX value where the MAE is smallest\n",
    "minMaeIDX = dataFrame.loc[dataFrame['MAE']==dataFrame['MAE'].min()].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataFrame.iloc[minMaeIDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minMaeAWS = dataFrame.iloc[minMaeIDX]['MAE']\n",
    "minR2AWS = dataFrame.iloc[minMaeIDX]['R2Score']\n",
    "modelNameAWS = \"MLP_64_Std\"\n",
    "posAWS = minMaeIDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_MLP = dataFrame['TrainHistory'][minMaeIDX]\n",
    "#Plot train vs validation\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(history_MLP.history['loss'])\n",
    "plt.plot(history_MLP.history['val_loss'])\n",
    "plt.title('Validation vs Train loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_MLP_std = dataFrame['TrainPredictions'][minMaeIDX]\n",
    "# Plot prediction vs original\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.scatter(range(y_test_all_std.shape[0]),y_test_all_std,label=\"Original Data\", alpha=0.6, c='red')\n",
    "plt.scatter(range(y_pred_MLP_std.shape[0]),y_pred_MLP_std,label=\"Predicted Data\", \n",
    "            alpha=0.6, c='black')\n",
    "plt.ylabel('Total Messages')\n",
    "plt.xlabel('Records')\n",
    "plt.title('MLP Std Model for X_test dataset prediction vs original')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataFrame128 = pd.DataFrame(columns=colList)\n",
    "layerSize = 128\n",
    "#loops = 15\n",
    "dataFrame128 = createModel(dataFrame128, layerSize, loops, \n",
    "                        y_train_scaled_std_all, X_train_all_std,\n",
    "                        y_test_all_std, X_test_all_std, \n",
    "                        targetStdScalerAll, 'all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataFrame128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the IDX value where the MAE is smallest\n",
    "minMaeIDX = dataFrame128.loc[dataFrame128['MAE']==dataFrame128['MAE'].min()].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataFrame128.iloc[minMaeIDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataFrame128.iloc[minMaeIDX]['MAE'] < minMaeAWS :\n",
    "    minMaeAWS = dataFrame128.iloc[minMaeIDX]['MAE']\n",
    "    minR2AWS = dataFrame128.iloc[minMaeIDX]['R2Score']\n",
    "    modelNameAWS = \"MLP_128_Std\"\n",
    "    posAWS = minMaeIDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot train vs validation\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "#plt.plot(100*dataFrame128['R2Score'])\n",
    "plt.plot(dataFrame128['MAE'])\n",
    "#plt.plot(dataFrame128['MSE'])\n",
    "plt.title('Training Scores MLP')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Iteration')\n",
    "plt.legend(['MAE'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_MLP = dataFrame128['TrainHistory'][minMaeIDX]\n",
    "#Plot train vs validation\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "plt.plot(history_MLP.history['loss'])\n",
    "plt.plot(history_MLP.history['val_loss'])\n",
    "plt.title('Validation vs Train loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_MLP_std = dataFrame128['TrainPredictions'][minMaeIDX]\n",
    "# Plot prediction vs original\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "plt.scatter(range(y_test_all_std.shape[0]),y_test_all_std,label=\"Original Data\", alpha=0.6, c='red')\n",
    "plt.scatter(range(y_pred_MLP_std.shape[0]),y_pred_MLP_std,label=\"Predicted Data\", \n",
    "            alpha=0.6, c='black')\n",
    "plt.ylabel('Total Messages')\n",
    "plt.xlabel('Records')\n",
    "plt.title('MLP Std Model for X_test dataset prediction vs original')\n",
    "plt.rc('legend', fontsize=18)\n",
    "plt.legend(loc='best', bbox_to_anchor=(0.75, 0, 0.25, 0.25))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataFrame32 = pd.DataFrame(columns=colList)\n",
    "layerSize = 32\n",
    "#loops = 15\n",
    "dataFrame32 = createModel(dataFrame32, layerSize, loops, \n",
    "                        y_train_scaled_std_all, X_train_all_std,\n",
    "                        y_test_all_std, X_test_all_std, \n",
    "                        targetStdScalerAll, 'all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the IDX value where the MAE is smallest\n",
    "minMaeIDX = dataFrame32.loc[dataFrame32['MAE']==dataFrame32['MAE'].min()].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataFrame32.iloc[minMaeIDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataFrame32.iloc[minMaeIDX]['MAE'] < minMaeAWS :\n",
    "    minMaeAWS = dataFrame32.iloc[minMaeIDX]['MAE']\n",
    "    minR2AWS = dataFrame32.iloc[minMaeIDX]['R2Score']\n",
    "    modelNameAWS = \"MLP_32_Std\"\n",
    "    posAWS = minMaeIDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot train vs validation\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "#plt.plot(100*dataFrame32['R2Score'])\n",
    "plt.plot(dataFrame32['MAE'])\n",
    "#plt.plot(dataFrame32['MSE'])\n",
    "plt.title('Training Scores MLP')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Iteration')\n",
    "plt.legend(['MAE'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_MLP = dataFrame32['TrainHistory'][minMaeIDX]\n",
    "#Plot train vs validation\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "plt.plot(history_MLP.history['loss'])\n",
    "plt.plot(history_MLP.history['val_loss'])\n",
    "plt.title('Validation vs Train loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_MLP_std = dataFrame32['TrainPredictions'][minMaeIDX]\n",
    "# Plot prediction vs original\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "plt.scatter(range(y_test_all_std.shape[0]),y_test_all_std,label=\"Original Data\", alpha=0.6, c='red')\n",
    "plt.scatter(range(y_pred_MLP_std.shape[0]),y_pred_MLP_std,label=\"Predicted Data\", \n",
    "            alpha=0.6, c='black')\n",
    "plt.ylabel('Total Messages')\n",
    "plt.xlabel('Records')\n",
    "plt.title('MLP Std Model for X_test dataset prediction vs original')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Base Std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataFrame32_1 = pd.DataFrame(columns=colList)\n",
    "layerSize = 32\n",
    "#loops = 15\n",
    "dataFrame32_1 = createModel(dataFrame32_1, layerSize, loops, \n",
    "                        y_train_scaled_std_base, X_train_base_std,\n",
    "                        y_test1_summaryL1Z2_std, X_test1_summaryL1Z2_std, \n",
    "                        targetStdScalerBase, 'base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame32_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the IDX value where the MAE is smallest\n",
    "minMaeIDX = dataFrame32_1.loc[dataFrame32_1['MAE']==dataFrame32_1['MAE'].min()].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataFrame32_1.iloc[minMaeIDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if dataFrame32_1.iloc[minMaeIDX]['MAE'] < minMaeAWS :\n",
    "minMaeAWS_base = dataFrame32_1.iloc[minMaeIDX]['MAE']\n",
    "minR2AWS_base = dataFrame32_1.iloc[minMaeIDX]['R2Score']\n",
    "modelNameAWS_base = \"MLP_32_Base_Std\"\n",
    "posAWS_base = minMaeIDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot train vs validation\n",
    "plt.figure(figsize=(20,10))\n",
    "#plt.plot(100*dataFrame32['R2Score'])\n",
    "plt.plot(dataFrame32_1['MAE'])\n",
    "#plt.plot(dataFrame32['MSE'])\n",
    "plt.title('Training Scores MLP')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Iteration')\n",
    "plt.legend(['MAE'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_MLP = dataFrame32_1['TrainHistory'][minMaeIDX]\n",
    "#Plot train vs validation\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(history_MLP.history['loss'])\n",
    "plt.plot(history_MLP.history['val_loss'])\n",
    "plt.title('Validation vs Train loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_MLP_std = dataFrame32_1['TrainPredictions'][minMaeIDX]\n",
    "# Plot prediction vs original\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.scatter(range(y_test1_summaryL1Z2_std.shape[0]),y_test1_summaryL1Z2_std,label=\"Original Data\", alpha=0.6, c='red')\n",
    "plt.scatter(range(y_pred_MLP_std.shape[0]),y_pred_MLP_std,label=\"Predicted Data\", \n",
    "            alpha=0.6, c='black')\n",
    "plt.ylabel('Total Messages')\n",
    "plt.xlabel('Records')\n",
    "plt.title('MLP Std Model for X_test dataset prediction vs original')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataFrame64_1 = pd.DataFrame(columns=colList)\n",
    "layerSize = 64\n",
    "#loops = 15\n",
    "dataFrame64_1 = createModel(dataFrame64_1, layerSize, loops, \n",
    "                        y_train_scaled_std_base, X_train_base_std,\n",
    "                        y_test1_summaryL1Z2_std, X_test1_summaryL1Z2_std, \n",
    "                        targetStdScalerBase, 'base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the IDX value where the MAE is smallest\n",
    "minMaeIDX = dataFrame64_1.loc[dataFrame64_1['MAE']==dataFrame64_1['MAE'].min()].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame64_1.iloc[minMaeIDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataFrame64_1.iloc[minMaeIDX]['MAE'] < minMaeAWS_base :\n",
    "    minMaeAWS_base = dataFrame64_1.iloc[minMaeIDX]['MAE']\n",
    "    minR2AWS_base = dataFrame64_1.iloc[minMaeIDX]['R2Score']\n",
    "    modelNameAWS_base = \"MLP_64_Base_Std\"\n",
    "    posAWS_base = minMaeIDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot train vs validation\n",
    "plt.figure(figsize=(20,10))\n",
    "#plt.plot(100*dataFrame32['R2Score'])\n",
    "plt.plot(dataFrame64_1['MAE'])\n",
    "#plt.plot(dataFrame32['MSE'])\n",
    "plt.title('Training Scores MLP')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Iteration')\n",
    "plt.legend(['MAE'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_MLP = dataFrame64_1['TrainHistory'][minMaeIDX]\n",
    "#Plot train vs validation\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(history_MLP.history['loss'])\n",
    "plt.plot(history_MLP.history['val_loss'])\n",
    "plt.title('Validation vs Train loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_MLP_Std = dataFrame64_1['TrainPredictions'][minMaeIDX]\n",
    "# Plot prediction vs original\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.scatter(range(y_test1_summaryL1Z2_std.shape[0]),y_test1_summaryL1Z2_std,label=\"Original Data\", alpha=0.6, c='black')\n",
    "plt.scatter(range(y_pred_MLP_std.shape[0]),y_pred_MLP_std,label=\"Predicted Data\", \n",
    "            alpha=0.6, c='red')\n",
    "plt.ylabel('Total Messages')\n",
    "plt.xlabel('Records')\n",
    "plt.title('MLP Std Model for X_test dataset prediction vs original')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataFrame128_1 = pd.DataFrame(columns=colList)\n",
    "layerSize = 128\n",
    "#loops = 15\n",
    "dataFrame128_1 = createModel(dataFrame128_1, layerSize, loops, \n",
    "                        y_train_scaled_std_base, X_train_base_std,\n",
    "                        y_test1_summaryL1Z2_std, X_test1_summaryL1Z2_std, \n",
    "                        targetStdScalerBase, 'base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the IDX value where the MAE is smallest\n",
    "minMaeIDX = dataFrame128_1.loc[dataFrame128_1['MAE']==dataFrame128_1['MAE'].min()].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame128_1.iloc[minMaeIDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataFrame128_1.iloc[minMaeIDX]['MAE'] < minMaeAWS_base :\n",
    "    minMaeAWS_base = dataFrame128_1.iloc[minMaeIDX]['MAE']\n",
    "    minR2AWS_base = dataFrame128_1.iloc[minMaeIDX]['R2Score']\n",
    "    modelNameAWS_base = \"MLP_64_Base_Std\"\n",
    "    posAWS_base = minMaeIDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot train vs validation\n",
    "plt.figure(figsize=(20,10))\n",
    "#plt.plot(100*dataFrame32['R2Score'])\n",
    "plt.plot(dataFrame128_1['MAE'])\n",
    "#plt.plot(dataFrame32['MSE'])\n",
    "plt.title('Training Scores MLP')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Iteration')\n",
    "plt.legend(['MAE'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_MLP = dataFrame128_1['TrainHistory'][minMaeIDX]\n",
    "#Plot train vs validation\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(history_MLP.history['loss'])\n",
    "plt.plot(history_MLP.history['val_loss'])\n",
    "plt.title('Validation vs Train loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_MLP_Std = dataFrame128_1['TrainPredictions'][minMaeIDX]\n",
    "# Plot prediction vs original\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.scatter(range(y_test1_summaryL1Z2_std.shape[0]),y_test1_summaryL1Z2_std,label=\"Original Data\", alpha=0.6, c='black')\n",
    "plt.scatter(range(y_pred_MLP_std.shape[0]),y_pred_MLP_std,label=\"Predicted Data\", \n",
    "            alpha=0.6, c='red')\n",
    "plt.ylabel('Total Messages')\n",
    "plt.xlabel('Records')\n",
    "plt.title('MLP Std Model for X_test dataset prediction vs original')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minMaeAWS, minR2AWS, modelNameAWS, posAWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minMaeAWS_base, minR2AWS_base, modelNameAWS_base, posAWS_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-CPU",
   "language": "python",
   "name": "tf-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
