{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"standardScalerSetup.ipynb\"\n",
    "%run \"minMaxScalerSetup.ipynb\"\n",
    "#loads all the preprocessing libraries and prepares the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM models\n",
    "# input shape format is [samples, timesteps, features]\n",
    "# base has [30, 1, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_to_lstm(df):\n",
    "    X = np.array(df)\n",
    "    return np.reshape(X, (X.shape[0], 1, X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the datasets for LSTM processing\n",
    "lstm_X_test1_summaryL1Z2_std = format_to_lstm(X_test1_summaryL1Z2_std)\n",
    "lstm_X_test1_summaryL1Z2_minMax = format_to_lstm(X_test1_summaryL1Z2_minMax)\n",
    "lstm_X_train_base_std = format_to_lstm(X_train_base_std)\n",
    "lstm_X_train_base_minMax = format_to_lstm(X_train_base_minMax)\n",
    "lstm_X_train_all_std = format_to_lstm(X_train_all_std)\n",
    "lstm_X_train_all_minMax = format_to_lstm(X_train_all_minMax)\n",
    "lstm_X_test_all_std = format_to_lstm(X_test_all_std)\n",
    "lstm_X_test_all_minMax = format_to_lstm(X_test_all_minMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_X_test1_summaryL1Z2_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_X_test_all_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_X_train_base_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Nets imports\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten \n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, Adadelta, Adagrad, Adamax, SGD\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define LSTM model\n",
    "#input shape for the LSTM is (number_steps x number_features)\n",
    "\n",
    "def lstm_model_mse(layerSize, numSteps, numFeatures):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(layerSize, return_sequences=True, input_shape=(numSteps,numFeatures),\n",
    "                    #kernel_regularizer=l1(0.01), bias_regularizer=l1(0.01),\n",
    "                    #kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01),\n",
    "                    kernel_regularizer=l1_l2(0.01), bias_regularizer=l1_l2(0.01),                   \n",
    "                   dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(LSTM(layerSize, input_shape=(numSteps,numFeatures), \n",
    "                    #kernel_regularizer=l1(0.01), bias_regularizer=l1(0.01),\n",
    "                    #kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01),\n",
    "                    kernel_regularizer=l1_l2(0.01), bias_regularizer=l1_l2(0.01),\n",
    "                   dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "    model.add(Dense(layerSize, kernel_initializer='normal',\n",
    "                    #kernel_regularizer=l1(0.01), bias_regularizer=l1(0.01),\n",
    "                    #kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01),\n",
    "                    kernel_regularizer=l1_l2(0.01), bias_regularizer=l1_l2(0.01),\n",
    "                    activation=custom_activation))\n",
    "    #model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(1, kernel_initializer='normal', \n",
    "                    #kernel_regularizer=l1(0.01), bias_regularizer=l1(0.01),\n",
    "                    #kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01),\n",
    "                    kernel_regularizer=l1_l2(0.01), bias_regularizer=l1_l2(0.01),\n",
    "                    activation='linear'))\n",
    "\n",
    "    model.compile(Adam(lr=0.001),\n",
    "              loss='mse',\n",
    "              metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "def lstm_model_mae(layerSize, numSteps, numFeatures):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(layerSize, return_sequences=True, input_shape=(numSteps,numFeatures),\n",
    "                    #kernel_regularizer=l1(0.01), bias_regularizer=l1(0.01),\n",
    "                    #kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01),\n",
    "                    kernel_regularizer=l1_l2(0.01), bias_regularizer=l1_l2(0.01),                   \n",
    "                   dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(LSTM(layerSize, input_shape=(numSteps,numFeatures), \n",
    "                    #kernel_regularizer=l1(0.01), bias_regularizer=l1(0.01),\n",
    "                    #kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01),\n",
    "                    kernel_regularizer=l1_l2(0.01), bias_regularizer=l1_l2(0.01),\n",
    "                   dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "    model.add(Dense(layerSize, kernel_initializer='normal',\n",
    "                    #kernel_regularizer=l1(0.01), bias_regularizer=l1(0.01),\n",
    "                    #kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01),\n",
    "                    kernel_regularizer=l1_l2(0.01), bias_regularizer=l1_l2(0.01),\n",
    "                    activation=custom_activation))\n",
    "    #model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(1, kernel_initializer='normal', \n",
    "                    #kernel_regularizer=l1(0.01), bias_regularizer=l1(0.01),\n",
    "                    #kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01),\n",
    "                    kernel_regularizer=l1_l2(0.01), bias_regularizer=l1_l2(0.01),\n",
    "                    activation='linear'))\n",
    "    \n",
    "    model.compile(Adam(lr=0.001),\n",
    "              loss='mae',\n",
    "              metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verboseLevel=0\n",
    "validationSplit=0.2\n",
    "batchSize=30\n",
    "epochs=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback preparation\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.5,\n",
    "                              patience=2,\n",
    "                              verbose=verboseLevel,\n",
    "                              mode='min',\n",
    "                              min_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_lstm_std_base_mae = ModelCheckpoint(\"lstm_model_std_base_mae_L1_L2.h5\",\n",
    "                             monitor='val_loss',\n",
    "                             verbose=verboseLevel,\n",
    "                             save_best_only=True,\n",
    "                             mode='min')\n",
    "\n",
    "callbacks_list_lstm_std_base_mae = [checkpoint_lstm_std_base_mae, reduce_lr]\n",
    "\n",
    "checkpoint_lstm_std_base_mse = ModelCheckpoint(\"lstm_model_std_base_mse_L1_L2.h5\",\n",
    "                             monitor='val_loss',\n",
    "                             verbose=verboseLevel,\n",
    "                             save_best_only=True,\n",
    "                             mode='min')\n",
    "\n",
    "callbacks_list_lstm_std_base_mse = [checkpoint_lstm_std_base_mse, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layerSize = 64\n",
    "numSteps = 1\n",
    "numFeatures = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_std_base_mae_model = lstm_model_mae(layerSize=layerSize, numSteps=numSteps, numFeatures=numFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_std_base_mae_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_std_base_mse_model = lstm_model_mse(layerSize=layerSize, numSteps=numSteps, numFeatures=numFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history_LSTM_std_base_mse = lstm_std_base_mse_model.fit(lstm_X_train_base_std, y_train_scaled_std_base,\n",
    "                                batch_size=5, \n",
    "                                validation_split=validationSplit, \n",
    "                                epochs=epochs, verbose=verboseLevel,\n",
    "                                callbacks=callbacks_list_lstm_std_base_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history_LSTM_std_base_mae = lstm_std_base_mae_model.fit(lstm_X_train_base_std, y_train_scaled_std_base,\n",
    "                                batch_size=5, \n",
    "                                validation_split=validationSplit, \n",
    "                                epochs=epochs, verbose=verboseLevel,\n",
    "                                callbacks=callbacks_list_lstm_std_base_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# LSTM STD BASE MAE\n",
    "#################################################\n",
    "#Reload the best saved model\n",
    "lstm_std_base_mae_model_new = load_model('lstm_model_std_base_mae_L1_L2.h5')\n",
    "#Predict\n",
    "y_pred_LSTM_base_std_L1Z2_MAE = lstm_std_base_mae_model_new.predict(lstm_X_test1_summaryL1Z2_std)\n",
    "pred = targetStdScalerBase.inverse_transform(y_pred_LSTM_base_std_L1Z2_MAE)\n",
    "#Show results\n",
    "print(metrics.r2_score(y_test1_summaryL1Z2_std, pred))\n",
    "print(metrics.mean_absolute_error(y_test1_summaryL1Z2_std, pred))\n",
    "print(metrics.mean_squared_error(y_test1_summaryL1Z2_std, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minMaeAWS_base = metrics.mean_absolute_error(y_test1_summaryL1Z2_std, pred)\n",
    "minR2AWS_base = metrics.r2_score(y_test1_summaryL1Z2_std, pred)\n",
    "modelNameAWS_base = \"LSTM_STD_BASE_MAE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot train vs validation\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(history_LSTM_std_base_mae.history['loss'])\n",
    "plt.plot(history_LSTM_std_base_mae.history['val_loss'])\n",
    "plt.title('LSTM MAE STD ALL model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot prediction vs original\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.scatter(range(y_test1_summaryL1Z2_std.shape[0]),y_test1_summaryL1Z2_std,label=\"Original Data\", alpha=0.6, c='black')\n",
    "plt.scatter(range(pred.shape[0]),pred,label=\"Predicted Data\", \n",
    "            alpha=0.6, c='red')\n",
    "plt.ylabel('Total Messages')\n",
    "plt.xlabel('Records')\n",
    "plt.title('LSTM1 MAE Std Base Model for L1Z2_test dataset prediction vs original')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# LSTM STD BASE MSE\n",
    "#################################################\n",
    "#Reload the best saved model\n",
    "lstm_std_base_mse_model_new = load_model('lstm_model_std_base_mse_L1_L2.h5')\n",
    "#Predict\n",
    "y_pred_LSTM_base_std_L1Z2_MSE = lstm_std_base_mse_model_new.predict(lstm_X_test1_summaryL1Z2_std)\n",
    "pred = targetStdScalerBase.inverse_transform(y_pred_LSTM_base_std_L1Z2_MSE)\n",
    "#Show results\n",
    "print(metrics.r2_score(y_test1_summaryL1Z2_std, pred))\n",
    "print(metrics.mean_absolute_error(y_test1_summaryL1Z2_std, pred))\n",
    "print(metrics.mean_squared_error(y_test1_summaryL1Z2_std, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if metrics.mean_absolute_error(y_test1_summaryL1Z2_std, pred) < minMaeAWS_base:\n",
    "    minMaeAWS_base = metrics.mean_absolute_error(y_test1_summaryL1Z2_std, pred)\n",
    "    minR2AWS_base = metrics.r2_score(y_test1_summaryL1Z2_std, pred)\n",
    "    modelNameAWS_base = \"LSTM_STD_BASE_MSE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot train vs validation\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(history_LSTM_std_base_mse.history['loss'])\n",
    "plt.plot(history_LSTM_std_base_mse.history['val_loss'])\n",
    "plt.title('LSTM MSE STD ALL model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prediction vs original\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.scatter(range(y_test1_summaryL1Z2_std.shape[0]),y_test1_summaryL1Z2_std,label=\"Original Data\", alpha=0.6, c='black')\n",
    "plt.scatter(range(pred.shape[0]),pred,label=\"Predicted Data\", \n",
    "            alpha=0.6, c='red')\n",
    "plt.ylabel('Total Messages')\n",
    "plt.xlabel('Records')\n",
    "plt.title('LSTM1 MSE Std Base Model for L1Z2_test dataset prediction vs original')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MinMax Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_lstm_minMax_base_mae = ModelCheckpoint(\"lstm_model_minMax_base_mae_L1_L2.h5\",\n",
    "                             monitor='val_loss',\n",
    "                             verbose=verboseLevel,\n",
    "                             save_best_only=True,\n",
    "                             mode='min')\n",
    "\n",
    "callbacks_list_lstm_minMax_base_mae = [checkpoint_lstm_minMax_base_mae, reduce_lr]\n",
    "\n",
    "checkpoint_lstm_minMax_base_mse = ModelCheckpoint(\"lstm_model_minMax_base_mse_L1_L2.h5\",\n",
    "                             monitor='val_loss',\n",
    "                             verbose=verboseLevel,\n",
    "                             save_best_only=True,\n",
    "                             mode='min')\n",
    "\n",
    "callbacks_list_lstm_minMax_base_mse = [checkpoint_lstm_minMax_base_mse, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_minMax_base_mae_model = lstm_model_mae(layerSize=layerSize, numSteps=numSteps, numFeatures=numFeatures)\n",
    "lstm_minMax_base_mse_model = lstm_model_mse(layerSize=layerSize, numSteps=numSteps, numFeatures=numFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history_LSTM_minMax_base_mae = lstm_minMax_base_mae_model.fit(lstm_X_train_base_minMax, y_train_scaled_minMax_base,\n",
    "                                batch_size=5, \n",
    "                                validation_split=validationSplit, \n",
    "                                epochs=epochs, verbose=verboseLevel,\n",
    "                                callbacks=callbacks_list_lstm_minMax_base_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history_LSTM_minMax_base_mse = lstm_minMax_base_mse_model.fit(lstm_X_train_base_minMax, y_train_scaled_minMax_base,\n",
    "                                batch_size=5, \n",
    "                                validation_split=validationSplit, \n",
    "                                epochs=epochs, verbose=verboseLevel,\n",
    "                                callbacks=callbacks_list_lstm_minMax_base_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# LSTM MinMax BASE MAE\n",
    "#################################################\n",
    "#Reload the best saved model\n",
    "lstm_minMax_base_mae_model_new = load_model('lstm_model_minMax_base_mae_L1_L2.h5')\n",
    "#Predict\n",
    "y_pred_LSTM_base_minMax_L1Z2_MAE = lstm_minMax_base_mae_model_new.predict(lstm_X_test1_summaryL1Z2_minMax)\n",
    "pred = targetMinMaxScalerBase.inverse_transform(y_pred_LSTM_base_minMax_L1Z2_MAE)\n",
    "#Show results\n",
    "print(metrics.r2_score(y_test1_summaryL1Z2_minMax, pred))\n",
    "print(metrics.mean_absolute_error(y_test1_summaryL1Z2_minMax, pred))\n",
    "print(metrics.mean_squared_error(y_test1_summaryL1Z2_minMax, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if metrics.mean_absolute_error(y_test1_summaryL1Z2_minMax, pred) < minMaeAWS_base:\n",
    "    minMaeAWS_base = metrics.mean_absolute_error(y_test1_summaryL1Z2_minMax, pred)\n",
    "    minR2AWS_base = metrics.r2_score(y_test1_summaryL1Z2_minMax, pred)\n",
    "    modelNameAWS_base = \"LSTM_MinMax_BASE_MAE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot train vs validation\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(history_LSTM_minMax_base_mae.history['loss'])\n",
    "plt.plot(history_LSTM_minMax_base_mae.history['val_loss'])\n",
    "plt.title('LSTM MAE MinMax ALL model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot prediction vs original\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.scatter(range(y_test1_summaryL1Z2_minMax.shape[0]),y_test1_summaryL1Z2_minMax,label=\"Original Data\", alpha=0.6, c='black')\n",
    "plt.scatter(range(pred.shape[0]),pred,label=\"Predicted Data\", \n",
    "            alpha=0.6, c='red')\n",
    "plt.ylabel('Total Messages')\n",
    "plt.xlabel('Records')\n",
    "plt.title('LSTM1 MAE MinMax Base Model for L1Z2_test dataset prediction vs original')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# LSTM MinMax BASE MSE\n",
    "#################################################\n",
    "#Reload the best saved model\n",
    "lstm_minMax_base_mse_model_new = load_model('lstm_model_minMax_base_mse_L1_L2.h5')\n",
    "#Predict\n",
    "y_pred_LSTM_base_minMax_L1Z2_MSE = lstm_minMax_base_mse_model_new.predict(lstm_X_test1_summaryL1Z2_minMax)\n",
    "pred = targetMinMaxScalerBase.inverse_transform(y_pred_LSTM_base_minMax_L1Z2_MSE)\n",
    "#Show results\n",
    "print(metrics.r2_score(y_test1_summaryL1Z2_minMax, pred))\n",
    "print(metrics.mean_absolute_error(y_test1_summaryL1Z2_minMax, pred))\n",
    "print(metrics.mean_squared_error(y_test1_summaryL1Z2_minMax, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if metrics.mean_absolute_error(y_test1_summaryL1Z2_minMax, pred) < minMaeAWS_base:\n",
    "    minMaeAWS_base = metrics.mean_absolute_error(y_test1_summaryL1Z2_minMax, pred)\n",
    "    minR2AWS_base = metrics.r2_score(y_test1_summaryL1Z2_minMax, pred)\n",
    "    modelNameAWS_base = \"LSTM_MinMax_BASE_MSE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot train vs validation\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(history_LSTM_minMax_base_mse.history['loss'])\n",
    "plt.plot(history_LSTM_minMax_base_mse.history['val_loss'])\n",
    "plt.title('LSTM MSE MinMax Base model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot prediction vs original\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.scatter(range(y_test1_summaryL1Z2_minMax.shape[0]),y_test1_summaryL1Z2_minMax,label=\"Original Data\", alpha=0.6, c='black')\n",
    "plt.scatter(range(pred.shape[0]),pred,label=\"Predicted Data\", \n",
    "            alpha=0.6, c='red')\n",
    "plt.ylabel('Total Messages')\n",
    "plt.xlabel('Records')\n",
    "plt.title('LSTM1 L2 MinMax Base Model for L1Z2_test dataset prediction vs original')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STD ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_lstm_std_all_mae = ModelCheckpoint(\"lstm_model_std_all_mae_L1_L2.h5\",\n",
    "                             monitor='val_loss',\n",
    "                             verbose=verboseLevel,\n",
    "                             save_best_only=True,\n",
    "                             mode='min')\n",
    "\n",
    "callbacks_list_lstm_std_all_mae = [checkpoint_lstm_std_all_mae, reduce_lr]\n",
    "\n",
    "checkpoint_lstm_std_all_mse = ModelCheckpoint(\"lstm_model_std_all_mse_L1_L2.h5\",\n",
    "                             monitor='val_loss',\n",
    "                             verbose=verboseLevel,\n",
    "                             save_best_only=True,\n",
    "                             mode='min')\n",
    "\n",
    "callbacks_list_std_all_base_mse = [checkpoint_lstm_std_all_mse, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_std_all_mae_model = lstm_model_mae(layerSize=layerSize, numSteps=numSteps, numFeatures=numFeatures)\n",
    "lstm_std_all_mse_model = lstm_model_mse(layerSize=layerSize, numSteps=numSteps, numFeatures=numFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history_LSTM_std_all_mae = lstm_std_all_mae_model.fit(lstm_X_train_all_std, y_train_scaled_std_all,\n",
    "                                batch_size=batchSize, \n",
    "                                validation_split=validationSplit, \n",
    "                                epochs=epochs, verbose=verboseLevel,\n",
    "                                callbacks=callbacks_list_lstm_std_all_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history_LSTM_std_all_mse = lstm_std_all_mse_model.fit(lstm_X_train_all_std, y_train_scaled_std_all,\n",
    "                                batch_size=batchSize, \n",
    "                                validation_split=validationSplit, \n",
    "                                epochs=epochs, verbose=verboseLevel,\n",
    "                                callbacks=callbacks_list_std_all_base_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# LSTM STD ALL MAE\n",
    "#################################################\n",
    "#Reload the best saved model\n",
    "lstm_std_all_mae_model_new = load_model('lstm_model_std_all_mae_L1_L2.h5')\n",
    "#Predict\n",
    "y_pred_LSTM_std_all_mae = lstm_std_all_mae_model_new.predict(lstm_X_test_all_std)\n",
    "pred = targetStdScalerAll.inverse_transform(y_pred_LSTM_std_all_mae)\n",
    "#Show results\n",
    "print(metrics.r2_score(y_test_all_std, pred))\n",
    "print(metrics.mean_absolute_error(y_test_all_std, pred))\n",
    "print(metrics.mean_squared_error(y_test_all_std, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minMaeAWS = metrics.mean_absolute_error(y_test_all_std, pred)\n",
    "minR2AWS = metrics.r2_score(y_test_all_std, pred)\n",
    "modelNameAWS = \"LSTM_STD_ALL_MAE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot train vs validation\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(history_LSTM_std_all_mae.history['loss'])\n",
    "plt.plot(history_LSTM_std_all_mae.history['val_loss'])\n",
    "plt.title('LSTM MAE STD ALL model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot prediction vs original\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.scatter(range(y_test_all_std.shape[0]),y_test_all_std,label=\"Original Data\", alpha=0.6, c='black')\n",
    "plt.scatter(range(pred.shape[0]),pred,label=\"Predicted Data\", \n",
    "            alpha=0.6, c='red')\n",
    "plt.ylabel('Total Messages')\n",
    "plt.xlabel('Records')\n",
    "plt.title('LSTM1 MAE STD ALL Model for X_test dataset prediction vs original')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# LSTM STD ALL MSE\n",
    "#################################################\n",
    "#Reload the best saved model\n",
    "lstm_std_all_mse_model_new = load_model('lstm_model_std_all_mse_L1_L2.h5')\n",
    "#Predict\n",
    "y_pred_LSTM_std_all_mse = lstm_std_all_mse_model_new.predict(lstm_X_test_all_std)\n",
    "pred = targetStdScalerAll.inverse_transform(y_pred_LSTM_std_all_mse)\n",
    "#Show results\n",
    "print(metrics.r2_score(y_test_all_std, pred))\n",
    "print(metrics.mean_absolute_error(y_test_all_std, pred))\n",
    "print(metrics.mean_squared_error(y_test_all_std, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if metrics.mean_absolute_error(y_test_all_std, pred) < minMaeAWS:\n",
    "    minMaeAWS = metrics.mean_absolute_error(y_test_all_std, pred)\n",
    "    minR2AWS = metrics.r2_score(y_test_all_std, pred)\n",
    "    modelNameAWS = \"LSTM_STD_ALL_MSE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot train vs validation\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(history_LSTM_std_all_mse.history['loss'])\n",
    "plt.plot(history_LSTM_std_all_mse.history['val_loss'])\n",
    "plt.title('LSTM MSE STD ALL model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot prediction vs original\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.scatter(range(y_test_all_std.shape[0]),y_test_all_std,label=\"Original Data\", alpha=0.6, c='black')\n",
    "plt.scatter(range(pred.shape[0]),pred,label=\"Predicted Data\", \n",
    "            alpha=0.6, c='red')\n",
    "plt.ylabel('Total Messages')\n",
    "plt.xlabel('Records')\n",
    "plt.title('LSTM1 MSE STD ALL Model for X_test dataset prediction vs original')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MinMax All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_lstm_minMax_all_mae = ModelCheckpoint(\"lstm_model_minMax_all_mae_L1_L2.h5\",\n",
    "                             monitor='val_loss',\n",
    "                             verbose=verboseLevel,\n",
    "                             save_best_only=True,\n",
    "                             mode='min')\n",
    "\n",
    "callbacks_list_lstm_minMax_all_mae = [checkpoint_lstm_minMax_all_mae, reduce_lr]\n",
    "\n",
    "checkpoint_lstm_minMax_all_mse = ModelCheckpoint(\"lstm_model_minMax_all_mse_L1_L2.h5\",\n",
    "                             monitor='val_loss',\n",
    "                             verbose=verboseLevel,\n",
    "                             save_best_only=True,\n",
    "                             mode='min')\n",
    "\n",
    "callbacks_list_lstm_minMax_all_mse = [checkpoint_lstm_minMax_all_mse, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_minMax_all_mae_model = lstm_model_mae(layerSize=layerSize, numSteps=numSteps, numFeatures=numFeatures)\n",
    "lstm_minMax_all_mse_model = lstm_model_mse(layerSize=layerSize, numSteps=numSteps, numFeatures=numFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history_LSTM_minMax_all_mae = lstm_minMax_all_mae_model.fit(lstm_X_train_all_minMax, y_train_scaled_minMax_all,\n",
    "                                batch_size=batchSize, \n",
    "                                validation_split=validationSplit, \n",
    "                                epochs=epochs, verbose=verboseLevel,\n",
    "                                callbacks=callbacks_list_lstm_minMax_all_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history_LSTM_minMax_all_mse = lstm_minMax_all_mse_model.fit(lstm_X_train_all_minMax, y_train_scaled_minMax_all,\n",
    "                                batch_size=batchSize, \n",
    "                                validation_split=validationSplit, \n",
    "                                epochs=epochs, verbose=verboseLevel,\n",
    "                                callbacks=callbacks_list_lstm_minMax_all_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# LSTM MinMax ALL MAE\n",
    "#################################################\n",
    "#Reload the best saved model\n",
    "lstm_minMax_all_mae_model_new = load_model('lstm_model_minMax_all_mae_L1_L2.h5')\n",
    "#Predict\n",
    "y_pred_LSTM_minMax_all_mae = lstm_minMax_all_mae_model_new.predict(lstm_X_test_all_minMax)\n",
    "pred = targetMinMaxScalerAll.inverse_transform(y_pred_LSTM_minMax_all_mae)\n",
    "#Show results\n",
    "print(metrics.r2_score(y_test_all_minMax, pred))\n",
    "print(metrics.mean_absolute_error(y_test_all_minMax, pred))\n",
    "print(metrics.mean_squared_error(y_test_all_minMax, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if metrics.mean_absolute_error(y_test_all_minMax, pred) < minMaeAWS:\n",
    "    minMaeAWS = metrics.mean_absolute_error(y_test_all_minMax, pred)\n",
    "    minR2AWS = metrics.r2_score(y_test_all_minMax, pred)\n",
    "    modelNameAWS = \"LSTM_MinMax_ALL_MAE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot train vs validation\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(history_LSTM_minMax_all_mae.history['loss'])\n",
    "plt.plot(history_LSTM_minMax_all_mae.history['val_loss'])\n",
    "plt.title('LSTM L1 MinMax ALL model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot prediction vs original\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.scatter(range(y_test_all_minMax.shape[0]),y_test_all_minMax,label=\"Original Data\", alpha=0.6, c='black')\n",
    "plt.scatter(range(pred.shape[0]),pred,label=\"Predicted Data\", \n",
    "            alpha=0.6, c='red')\n",
    "plt.ylabel('Total Messages')\n",
    "plt.xlabel('Records')\n",
    "plt.title('LSTM MAE MinMax ALL Model for X_test dataset prediction vs original')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# LSTM MinMax ALL MSE\n",
    "#################################################\n",
    "#Reload the best saved model\n",
    "lstm_minMax_all_mse_model_new = load_model('lstm_model_minMax_all_mse_L1_L2.h5')\n",
    "#Predict\n",
    "y_pred_LSTM_minMax_all_mse = lstm_minMax_all_mse_model_new.predict(lstm_X_test_all_minMax)\n",
    "pred = targetMinMaxScalerAll.inverse_transform(y_pred_LSTM_minMax_all_mse)\n",
    "#Show results\n",
    "print(metrics.r2_score(y_test_all_minMax, pred))\n",
    "print(metrics.mean_absolute_error(y_test_all_minMax, pred))\n",
    "print(metrics.mean_squared_error(y_test_all_minMax, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if metrics.mean_absolute_error(y_test_all_minMax, pred) < minMaeAWS:\n",
    "    minMaeAWS = metrics.mean_absolute_error(y_test_all_minMax, pred)\n",
    "    minR2AWS = metrics.r2_score(y_test_all_minMax, pred)\n",
    "    modelNameAWS = \"LSTM_MinMax_ALL_MSE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot train vs validation\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(history_LSTM_minMax_all_mse.history['loss'])\n",
    "plt.plot(history_LSTM_minMax_all_mse.history['val_loss'])\n",
    "plt.title('LSTM MSE MinMax ALL model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot prediction vs original\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.scatter(range(y_test_all_minMax.shape[0]),y_test_all_minMax,label=\"Original Data\", alpha=0.6, c='black')\n",
    "plt.scatter(range(pred.shape[0]),pred,label=\"Predicted Data\", \n",
    "            alpha=0.6, c='red')\n",
    "plt.ylabel('Total Messages')\n",
    "plt.xlabel('Records')\n",
    "plt.title('LSTM MSE MinMax ALL Model for X_test dataset prediction vs original')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minMaeAWS_base, minR2AWS_base, modelNameAWS_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minMaeAWS, minR2AWS, modelNameAWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### END ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below is how to create multivariate time series with multiple steps from a single step multivariate series\n",
    "def format_to_lstm_nsteps(df_X, df_Y, n_steps=1):\n",
    "    X, y = list(), list()\n",
    "    df_X_a = df_X.to_numpy()\n",
    "    #df_Y_a = df_Y.to_numpy()\n",
    "    for i in range(len(df_X)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(df_X):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x = df_X_a[i:end_ix, :]\n",
    "        seq_y = df_Y[end_ix-1]\n",
    "        #print(f'Iteration: {i}, end_ix={end_ix}')\n",
    "        #print(f'seq_x={seq_x}')\n",
    "        #print(f'seq_y={seq_y}')\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lstm3, y_train_lstm3 = format_to_lstm_nsteps(X_train_base_std, y_train_scaled_std_base,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lstm3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_lstm3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm3_std_base_mae_model = lstm_model_mae(layerSize=layerSize, numSteps=3, numFeatures=numFeatures)\n",
    "lstm3_std_base_mse_model = lstm_model_mse(layerSize=layerSize, numSteps=3, numFeatures=numFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm3_std_base_mae_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_lstm3_std_base_mae = ModelCheckpoint(\"lstm3_model_std_base_mae_L1_L2.h5\",\n",
    "                             monitor='val_loss',\n",
    "                             verbose=verboseLevel,\n",
    "                             save_best_only=True,\n",
    "                             mode='min')\n",
    "\n",
    "callbacks_list_lstm3_std_base_mae = [checkpoint_lstm3_std_base_mae, reduce_lr]\n",
    "\n",
    "checkpoint_lstm3_std_base_mse = ModelCheckpoint(\"lstm3_model_std_base_mse_L1_L2.h5\",\n",
    "                             monitor='val_loss',\n",
    "                             verbose=verboseLevel,\n",
    "                             save_best_only=True,\n",
    "                             mode='min')\n",
    "\n",
    "callbacks_list_lstm3_std_base_mse = [checkpoint_lstm3_std_base_mse, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history_LSTM3_std_base_mae = lstm3_std_base_mae_model.fit(X_train_lstm3, y_train_lstm3,\n",
    "                                batch_size=5, \n",
    "                                validation_split=validationSplit, \n",
    "                                epochs=epochs, verbose=verboseLevel,\n",
    "                                callbacks=callbacks_list_lstm3_std_base_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history_LSTM3_std_base_mse = lstm3_std_base_mse_model.fit(X_train_lstm3, y_train_lstm3,\n",
    "                                batch_size=5, \n",
    "                                validation_split=validationSplit, \n",
    "                                epochs=epochs, verbose=verboseLevel,\n",
    "                                callbacks=callbacks_list_lstm3_std_base_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_lstm3, y_test_lstm3 = format_to_lstm_nsteps(X_test1_summaryL1Z2_std, y_test_scaled_std_base, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot train vs validation\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(history_LSTM3_std_base_mae.history['loss'])\n",
    "plt.plot(history_LSTM3_std_base_mae.history['val_loss'])\n",
    "plt.title('LSTM3_std_base MAE model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reload the best saved model\n",
    "lstm3_std_base_mae_model_new = load_model('lstm3_model_std_base_mae_L1_L2.h5')\n",
    "#Predict\n",
    "y_pred_LSTM3_base_std_L1Z2_mae = lstm3_std_base_mae_model_new.predict(X_test_lstm3)\n",
    "pred = targetStdScalerBase.inverse_transform(y_pred_LSTM3_base_std_L1Z2_mae)\n",
    "y_target = targetStdScalerBase.inverse_transform(y_test_lstm3)\n",
    "#Show results\n",
    "print(metrics.r2_score(y_target, pred))\n",
    "print(metrics.mean_absolute_error(y_target, pred))\n",
    "print(metrics.mean_squared_error(y_target, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prediction vs original\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.scatter(range(y_target.shape[0]),y_target,label=\"Original Data\", alpha=0.6, c='black')\n",
    "plt.scatter(range(pred.shape[0]),pred,label=\"Predicted Data\", \n",
    "            alpha=0.6, c='red')\n",
    "plt.ylabel('Total Messages')\n",
    "plt.xlabel('Records')\n",
    "plt.title('LSTM3 MAE Std Base Model for L1Z2_test dataset prediction vs original')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot train vs validation\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(history_LSTM3_std_base_mse.history['loss'])\n",
    "plt.plot(history_LSTM3_std_base_mse.history['val_loss'])\n",
    "plt.title('LSTM3_std_base MSE model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reload the best saved model\n",
    "lstm3_std_base_mse_model_new = load_model('lstm3_model_std_base_mse_L1_L2.h5')\n",
    "#Predict\n",
    "y_pred_LSTM_base_std_L1Z2_mse = lstm3_std_base_mse_model_new.predict(X_test_lstm3)\n",
    "pred = targetStdScalerBase.inverse_transform(y_pred_LSTM_base_std_L1Z2_mse)\n",
    "y_target = targetStdScalerBase.inverse_transform(y_test_lstm3)\n",
    "#Show results\n",
    "print(metrics.r2_score(y_target, pred))\n",
    "print(metrics.mean_absolute_error(y_target, pred))\n",
    "print(metrics.mean_squared_error(y_target, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prediction vs original\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.scatter(range(y_target.shape[0]),y_target,label=\"Original Data\", alpha=0.6, c='black')\n",
    "plt.scatter(range(pred.shape[0]),pred,label=\"Predicted Data\", \n",
    "            alpha=0.6, c='red')\n",
    "plt.ylabel('Total Messages')\n",
    "plt.xlabel('Records')\n",
    "plt.title('LSTM3 MSE Std Base Model for L1Z2_test dataset prediction vs original')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-CPU",
   "language": "python",
   "name": "tf-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
