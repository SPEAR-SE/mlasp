{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Nets imports\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten \n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, Adadelta, Adagrad, Adamax, SGD\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CNN Model ##########\n",
    "def format_to_cnn(df):\n",
    "    X = np.expand_dims(df, axis=2)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verboseLevel=0\n",
    "validationSplit=0.2\n",
    "batchSize=30\n",
    "epochs=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback preparation\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.5,\n",
    "                              patience=2,\n",
    "                              verbose=verboseLevel,\n",
    "                              mode='min',\n",
    "                              min_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createReduceRLObject():\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.5,\n",
    "                              patience=2,\n",
    "                              verbose=verboseLevel,\n",
    "                              mode='min',\n",
    "                              min_lr=0.001)\n",
    "    return reduce_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputSize = 9\n",
    "colList = ['HiddenLayers', 'R2Score', 'MAE', 'MSE', 'H5FileName', 'TrainHistory', 'TrainPredictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define CNN model\n",
    "def cnn_model_mae(inputSize=inputSize):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=6, activation=custom_activation,\n",
    "                     #kernel_regularizer=l1(0.01), bias_regularizer=l1(0.01),\n",
    "                     kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01),\n",
    "                     #kernel_regularizer=l1_l2(0.01), bias_regularizer=l1_l2(0.01),                   \n",
    "                     input_shape=(inputSize,1)))\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, \n",
    "                     #kernel_regularizer=l1(0.01), bias_regularizer=l1(0.01),\n",
    "                     kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01),\n",
    "                     #kernel_regularizer=l1_l2(0.01), bias_regularizer=l1_l2(0.01),                   \n",
    "                     activation=custom_activation))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, \n",
    "                    #kernel_regularizer=l1(0.01), bias_regularizer=l1(0.01),\n",
    "                    kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01),\n",
    "                    #kernel_regularizer=l1_l2(0.01), bias_regularizer=l1_l2(0.01),                   \n",
    "                    activation=custom_activation))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Dense(1, \n",
    "                    #kernel_regularizer=l1(0.01), bias_regularizer=l1(0.01),\n",
    "                    kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01),\n",
    "                    #kernel_regularizer=l1_l2(0.01), bias_regularizer=l1_l2(0.01),                   \n",
    "                    activation=\"linear\"))\n",
    "\n",
    "    optmzr=Adam(lr=0.001)\n",
    "    model.compile(optimizer=optmzr, loss='mae', metrics=['mae'])\n",
    "\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the datasets for CNN processing\n",
    "cnn_X_test1_summaryL1Z2_std = format_to_cnn(X_test1_summaryL1Z2_std)\n",
    "cnn_X_test1_summaryL1Z2_minMax = format_to_cnn(X_test1_summaryL1Z2_minMax)\n",
    "cnn_X_train_base_std = format_to_cnn(X_train_base_std)\n",
    "cnn_X_train_base_minMax = format_to_cnn(X_train_base_minMax)\n",
    "cnn_X_train_all_std = format_to_cnn(X_train_all_std)\n",
    "cnn_X_train_all_minMax = format_to_cnn(X_train_all_minMax)\n",
    "cnn_X_test_all_std = format_to_cnn(X_test_all_std)\n",
    "cnn_X_test_all_minMax = format_to_cnn(X_test_all_minMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_regr_std_base_mae = cnn_model_mae()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cnn_std_base_mae = ModelCheckpoint(\"cnn_model_std_base_mae_L2.h5\",\n",
    "                             monitor='val_loss',\n",
    "                             verbose=verboseLevel,\n",
    "                             save_best_only=True,\n",
    "                             mode='min')\n",
    "callbacks_list_cnn_std_base_mae = [checkpoint_cnn_std_base_mae, reduce_lr]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base STD MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history_CNN_std_base_mae = cnn_regr_std_base_mae.fit(cnn_X_train_base_std, y_train_scaled_std_base,\n",
    "                            batch_size=5, \n",
    "                            validation_split=0.2, \n",
    "                            epochs=epochs, verbose=verboseLevel,\n",
    "                            callbacks=callbacks_list_cnn_std_base_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_regr_std_base_mae_new =load_model('cnn_model_std_base_mae_L2.h5')\n",
    "#Predict\n",
    "y_pred_CNN_std_base_mae_scaled = cnn_regr_std_base_mae_new.predict(cnn_X_test1_summaryL1Z2_std)\n",
    "#reverse scaling to compare to true values\n",
    "pred = targetStdScalerBase.inverse_transform(y_pred_CNN_std_base_mae_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minMseAWS_base = metrics.mean_squared_error(y_test1_summaryL1Z2_std, pred)\n",
    "minRmseAWS_base = np.sqrt(minMseAWS_base)\n",
    "minMaeAWS_base = metrics.mean_absolute_error(y_test1_summaryL1Z2_std, pred)\n",
    "minR2AWS_base = metrics.r2_score(y_test1_summaryL1Z2_std, pred)\n",
    "modelNameAWS_base = \"CNN_STD_Base_MAE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = pd.DataFrame()\n",
    "delta = (1-(pd.DataFrame(pred)  / y_test1_summaryL1Z2_std.to_numpy()))*100\n",
    "delta = delta.rename(columns={delta.columns[0]: \"ThroughputDeltaDeviationPercentage\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpd_base = delta.median()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(test, pred):\n",
    "    x = 0\n",
    "    for i in range(len(test)):\n",
    "        x+=np.abs((test[test.columns[0]][i] - pred[i])/test[test.columns[0]][i])\n",
    "    return (x/len(test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_base = mape(y_test1_summaryL1Z2_std, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minMaeAWS_base, minR2AWS_base, modelNameAWS_base, minMseAWS_base, minRmseAWS_base"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-CPU",
   "language": "python",
   "name": "tf-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
