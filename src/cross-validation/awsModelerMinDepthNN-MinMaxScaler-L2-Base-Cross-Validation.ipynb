{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The purpose of this notebook is to find out what's the smallest network \n",
    "# that can be used to get max prediction score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Nets imports\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten \n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, Adadelta, Adagrad, Adamax, SGD\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verboseLevel=0\n",
    "validationSplit=0.2\n",
    "batchSize=30\n",
    "epochs=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback preparation\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.5,\n",
    "                              patience=2,\n",
    "                              verbose=verboseLevel,\n",
    "                              mode='min',\n",
    "                              min_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputSize = 9\n",
    "colList = ['HiddenLayers', 'R2Score', 'MAE', 'MSE', 'RMSE', 'H5FileName', 'TrainHistory', 'TrainPredictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Large Dataset first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(dataFrame, layerSize, loops, y_train, X_train, y_test, X_test, targetScaler, labelSet):\n",
    "    #print(f'Creating model using layer size = {layerSize} on set = {labelSet}.\\n')\n",
    "    i = loops\n",
    "    #print(f'Training on {i} hidden layers\\n')\n",
    "    model = Sequential()\n",
    "    model.add(Dense(layerSize, kernel_initializer='normal',\n",
    "                    #kernel_regularizer=l1(0.01), bias_regularizer=l1(0.01),\n",
    "                    kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01),\n",
    "                    #kernel_regularizer=l1_l2(0.01), bias_regularizer=l1_l2(0.01),\n",
    "                    input_dim=inputSize, activation=custom_activation))\n",
    "    for j in range(i):\n",
    "        model.add(Dense(layerSize, \n",
    "                        #kernel_regularizer=l1(0.01), bias_regularizer=l1(0.01),\n",
    "                        kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01),\n",
    "                        #kernel_regularizer=l1_l2(0.01), bias_regularizer=l1_l2(0.01),\n",
    "                        kernel_initializer='normal', activation=custom_activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1, kernel_initializer='normal', \n",
    "                    #kernel_regularizer=l1(0.01), bias_regularizer=l1(0.01),\n",
    "                    kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01),\n",
    "                    #kernel_regularizer=l1_l2(0.01), bias_regularizer=l1_l2(0.01),\n",
    "                    activation='linear'))\n",
    "\n",
    "    # only set this if GPU capabilities available\n",
    "    # model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    optmzr=Adam(lr=0.001)    \n",
    "    model.compile(optimizer=optmzr, loss='mae', metrics=['mae'])\n",
    "\n",
    "    model_h5_name = 'mlp_' + str(layerSize)+ '_' + str(i) + '_model_minMax_' + labelSet + '_L2.h5'\n",
    "    checkpoint_nn_minMax = ModelCheckpoint(model_h5_name,\n",
    "                             monitor='val_loss',\n",
    "                             verbose=verboseLevel,\n",
    "                             save_best_only=True,\n",
    "                             mode='min')\n",
    "    callbacks_list_nn_minMax = [checkpoint_nn_minMax, reduce_lr]\n",
    "\n",
    "    history_MLP_minMax = model.fit(X_train.to_numpy(), y_train,\n",
    "                                batch_size=batchSize, \n",
    "                                validation_split=validationSplit, \n",
    "                                epochs=epochs, verbose=verboseLevel,\n",
    "                                callbacks=callbacks_list_nn_minMax)\n",
    "\n",
    "    #reload the best model!\n",
    "    model_new = load_model(model_h5_name)\n",
    "    #Predict\n",
    "    y_pred_scaled = model_new.predict(X_test.to_numpy())\n",
    "    #Evaluate metrics\n",
    "    y_pred = targetScaler.inverse_transform(y_pred_scaled)\n",
    "    r2_score = metrics.r2_score(y_test, y_pred)\n",
    "    mae = metrics.mean_absolute_error(y_test, y_pred)\n",
    "    mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    #store values\n",
    "    row = [i, r2_score, mae, mse, rmse, model_h5_name, history_MLP_minMax, y_pred]\n",
    "    df = pd.DataFrame(np.array(row).reshape(1, len(colList)), columns=colList)\n",
    "    dataFrame = dataFrame.append(df, ignore_index=True)\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    del(model)\n",
    "    del(model_new)\n",
    "        \n",
    "    return dataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Base MinMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataFrame128_1 = pd.DataFrame(columns=colList)\n",
    "layerSize = 128\n",
    "loops = 3\n",
    "dataFrame128_1 = createModel(dataFrame128_1, layerSize, loops, \n",
    "                        y_train_scaled_minMax_base, X_train_base_minMax,\n",
    "                        y_test1_summaryL1Z2_minMax, X_test1_summaryL1Z2_minMax, \n",
    "                        targetMinMaxScalerBase, 'base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the IDX value where the MAE is smallest\n",
    "minMaeIDX = dataFrame128_1.loc[dataFrame128_1['MAE']==dataFrame128_1['MAE'].min()].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame128_1.iloc[minMaeIDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the scaler\n",
    "#joblib.dump(targetMinMaxScalerBase, 'targetTrainMinMaxScalerBase.pkl')\n",
    "#joblib.dump(minMaxScalerBase, 'trainMinMaxScalerBase.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if dataFrame128_1.iloc[minMaeIDX]['MAE'] < minMaeAWS_base :\n",
    "minMseAWS_base = dataFrame128_1.iloc[minMaeIDX]['MSE']\n",
    "minRmseAWS_base = dataFrame128_1.iloc[minMaeIDX]['RMSE']\n",
    "minMaeAWS_base = dataFrame128_1.iloc[minMaeIDX]['MAE']\n",
    "minR2AWS_base = dataFrame128_1.iloc[minMaeIDX]['R2Score']\n",
    "modelNameAWS_base = \"MLP_128_Base_MinMax\"\n",
    "posAWS_base = minMaeIDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_MLP_MinMax = dataFrame128_1['TrainPredictions'][minMaeIDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = pd.DataFrame()\n",
    "delta = (1-(pd.DataFrame(y_pred_MLP_MinMax)  / y_test1_summaryL1Z2_minMax.to_numpy()))*100\n",
    "delta = delta.rename(columns={delta.columns[0]: \"ThroughputDeltaDeviationPercentage\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpd_base = delta.median()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(test, pred):\n",
    "    x = 0\n",
    "    for i in range(len(test)):\n",
    "        x+=np.abs((test[test.columns[0]][i] - pred[i])/test[test.columns[0]][i])\n",
    "    return (x/len(test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_base = mape(y_test1_summaryL1Z2_minMax, y_pred_MLP_MinMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minMaeAWS_base, minR2AWS_base, modelNameAWS_base, posAWS_base, minMseAWS_base, minRmseAWS_base, mape_base, mpd_base"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-CPU",
   "language": "python",
   "name": "tf-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
